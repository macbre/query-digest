"""
This is a **dynamic code analysis tool** that processes last hour of **SQL queries logs generated by MediaWiki** in SJC prod environment and reports
those made by given feature

Usage:
  query_digest --path=<path> [ --csv ]

Example:
  query_digest --path=extensions/wikia/Wall
  query_digest --path=extensions/wikia/Wall --csv
"""
from __future__ import print_function

import docopt
import logging

from csv import DictWriter
from sys import stdout
from tabulate import tabulate

from digest.map_reduce import map_reduce
from digest.math import median
from digest.queries import get_sql_queries, normalize_query_log_entry, filter_query


def queries_reduce(_, values, sequence_len):
    """
    :type _ str
    :type values tuple[dict]
    :type sequence_len int
    :rtype dict
    """
    ret = values[0].copy()

    ret['count'] = len(values)
    ret['percentage'] = '{:.2f}%'.format(100. * ret['count'] / sequence_len)

    # calculate times stats
    times = [value.get('time') for value in values]

    ret['time_sum'] = sum(times)
    ret['time_median'] = median(times)

    # rows stats
    rows = [value.get('rows') for value in values]

    ret['rows_sum'] = sum(rows)
    ret['rows_median'] = median(rows)

    # get rid of item specific fields
    del ret['rows']
    del ret['time']

    return ret


def main():
    logger = logging.getLogger('query_digest')

    # handle command line options
    arguments = docopt.docopt(__doc__)
    logger.info("Got the following arguments: {}".format(arguments))

    path = arguments.get('--path')
    output_csv = arguments.get('--csv') is True

    logger.info('Digesting queries for "{}" path'.format(path))

    # run the reporter
    queries = tuple(map(normalize_query_log_entry, get_sql_queries(path)))
    queries = tuple(filter(filter_query, queries))

    if len(queries) == 0:
        raise Exception('No queries found for "{}" path'.format(path))

    logger.info('Processing {} queries...'.format(len(queries)))

    results = map_reduce(
        queries,
        map_func=lambda item: '{}-{}'.format(item.get('method'), item.get('source_host')),
        reduce_func=queries_reduce
    )

    logger.info('Got {} kinds of queries'.format(len(results)))

    # sort the results ordered by "time_sum" descending
    results_ordered = sorted(results, key=lambda (_, item): item['time_sum'], reverse=True)
    data = [entry for (_, entry) in results_ordered]

    report_header = 'Query digest for "{}" path, found {} queries'.format(path, len(queries))

    if output_csv:
        writer = DictWriter(f=stdout, fieldnames=data[0].keys())

        stdout.write('# {}\n'.format(report_header))
        writer.writeheader()
        writer.writerows(data)
    else:
        # @see https://pypi.python.org/pypi/tabulate
        print(report_header)
        print(tabulate(data, headers='keys', tablefmt='grid'))
        print('Note: times are in [ms], queries are normalized')
